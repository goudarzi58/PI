# <<<<<<< Modified: Logging Setup >>>>>>>
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

import io
import json
from pydoc import text
import fitz
import uuid
import boto3
from pathlib import Path
from datetime import datetime
from botocore.exceptions import ClientError
from PIL import Image, ImageDraw, ImageStat, ImageEnhance
from textractcaller.t_call import call_textract, Textract_Types
from textractoverlayer.t_overlay import DocumentDimensions, get_bounding_boxes
from textractprettyprinter.t_pretty_print import Textract_Pretty_Print, get_string
import warnings
warnings.filterwarnings('ignore')
import os
print("Current working directory:", os.getcwd())

# --- Load config.json to run locally ---
script_dir = os.path.dirname(os.path.abspath(__file__))
config_path = os.path.join(script_dir, 'config_local_run.json')

with open(config_path, 'r') as f:
    config = json.load(f)

# <<<<<<< Modified: Config Validation >>>>>>>
required_config_keys = ['aws', 'aws_prod', 'services']
for key in required_config_keys:
    if key not in config:
        logging.error(f"Missing required key in config: {key}")
        raise KeyError(f"Missing required key in config: {key}")

# Extract config variables
prompt_filename = config['prompt']['PROMPT_TEMPLATE']
with open(prompt_filename, 'r') as f:
    prompt_template = f.read()

aws_config = config['aws']
access_key = aws_config['access_key']
secret_key = aws_config['secret_key']
REGION = aws_config['region']
services_config = config['services']

aws_config_prod = config['aws_prod']
access_key_prod = aws_config_prod['access_key_prod']
secret_key_prod = aws_config_prod['secret_key_prod']
REGION_prod = aws_config_prod['region_prod']

s3_redacted_bucket = services_config['s3']['redacted_bucket_name']
s3_manual_review_bucket = services_config['s3']['manual_review_bucket']
s3_source_bucket_prod = services_config['s3_prod']['source_bucket_name_prod']
sqs_queue_url = services_config['sqs']['HITLReview_Queue']['Queue_URL']
sqs_batch_size = services_config['sqs']['HITLReview_Queue']['Batch_Size']
sovereign_variables = services_config['sovereign']['sovereign_variables']

# <<<<<<< Modified: AWS Initialization with Exception Handling >>>>>>>
try:
    s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name=REGION, verify=False)
    s3_prod = boto3.client('s3', aws_access_key_id=access_key_prod, aws_secret_access_key=secret_key_prod, region_name=REGION, verify=False)
    textract = boto3.client('textract', aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name=REGION, verify=False)
    dynamodb = boto3.resource('dynamodb', aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name=REGION, verify=False)
    sqs = boto3.client('sqs', aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name=REGION, verify=False)
    bedrock = boto3.client('bedrock-runtime', aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name=REGION, verify=False)
except Exception as e:
    logging.critical("Failed to initialize AWS clients", exc_info=True)
    raise SystemExit("Terminating due to AWS initialization error.")

metadata_json = {}

# <<<<<<< Added: Retry Function >>>>>>>
def retry_on_failure(func, retries=3, delay=5):
    for attempt in range(retries):
        try:
            return func()
        except ClientError as e:
            logging.warning(f"Retry {attempt+1} failed: {e}")
            if attempt < retries - 1:
                time.sleep(delay)
            else:
                raise

# <<<<<<< Modified: Read File From S3 With Retry >>>>>>>
def read_file_from_s3_prod(bucket_name, object_key):
    try:
        logging.info(f"Reading file from S3 bucket: {bucket_name}, key: {object_key}")
        return retry_on_failure(lambda: s3_prod.get_object(Bucket=bucket_name, Key=object_key)['Body'].read())
    except ClientError as e:
        logging.error("Error reading file from S3", exc_info=True)
        raise

# <<<<<<< Modified: Bedrock Error Handling >>>>>>>
def process_with_bedrock(text_pages, page_number):
    try:
        formatted_prompt = prompt_template.replace("{page_count}", str(len(text_pages)))
        messages = [{"role": "user", "content": [
            {"type": "text", "text": f"Use this context: {text_pages}"},
            {"type": "text", "text": formatted_prompt}]}]

        request = {
            "anthropic_version": services_config['bedrock']['anthropic_version'],
            "max_tokens": services_config['bedrock']['max_tokens'],
            "temperature": services_config['bedrock']['temperature'],
            "top_p": services_config['bedrock']['top_p'],
            "messages": messages
        }

        response = retry_on_failure(lambda: bedrock.invoke_model(
            modelId=services_config['bedrock']['model_id'], body=json.dumps(request)))

        response_body = response['body'].read().decode('utf-8')
        if not response_body:
            raise ValueError("Bedrock response body is empty")

        result = json.loads(response_body)
        text_response = result['content'][0]['text']
        json_text = text_response.split("</document_analysis>")[-1].strip()

        if json_text.startswith("```json"):
            json_text = json_text[len("```json"):].strip()
        if json_text.startswith("```"):
            json_text = json_text[len("```"):].strip()
        if json_text.endswith("```"):
            json_text = json_text[:-3].strip()

        parsed_json = json.loads(json_text)
        page_num = page_number + 1
        parsed_json_result = {"metadata": {"pages": [{str(page_num): parsed_json['metadata']['entities']}]} }
        return [parsed_json_result]

    except json.JSONDecodeError as e:
        logging.error("Failed to parse JSON from Bedrock response", exc_info=True)
        raise
    except Exception as e:
        logging.error("Unexpected error in Bedrock processing", exc_info=True)
        raise

# <<<<<<< Modified: Overlay Error Handling >>>>>>>
def apply_overlay_to_images(images, metadata_json):
    if len(images) != len(metadata_json['pages']):
        logging.error("Mismatch between image count and metadata pages")
        raise ValueError("Mismatch between image count and metadata pages")

    processed_images = []
    fill_color = (0, 0, 0, 255)
    for index, image in enumerate(images):
        if image.mode != 'RGBA':
            image = image.convert('RGBA')
        overlay = Image.new('RGBA', image.size, (0, 0, 0, 0))
        draw = ImageDraw.Draw(overlay)
        page_elements = metadata_json['pages'][index][str(index + 1)]
        redactions = [element['bbox'] for element in page_elements]
        for box in redactions:
            draw.rectangle(xy=[box[0], box[1], box[2], box[3]], fill=fill_color)
        image_with_highlights = Image.alpha_composite(image, overlay)
        processed_images.append(image_with_highlights)
    return processed_images

# <<<<<<< Modified: Main With Exception Handling >>>>>>>
def main():
    try:
        global dynamodb_table
        dynamodb_table = dynamodb.Table(services_config['dynamodb']['Metadata_Table_Name'])

        while True:
            messages = read_message_from_queue(sqs_queue_url, sqs_batch_size)
            if messages is None:
                logging.info("No Items in the queue")
                break

            for message in messages:
                partition_key = str(datetime.now().strftime('%Y-%m-%d'))
                sort_key = "jr_" + str(uuid.uuid4())
                object_key = message['Body']
                file_extension = Path(object_key).suffix.lower()

                if file_extension != '.pdf':
                    upsert_dynamoDB_status(partition_key, sort_key, object_key, "Error", "Unknown file extension")
                    delete_message_from_queue(sqs_queue_url, message)
                    continue

                upsert_dynamoDB_status(partition_key, sort_key, object_key, "Initializing", f"File {file_extension} file detected")
                logging.info(f"Initializing - File detected: {object_key}")

                try:
                    process_pdf(partition_key, sort_key, object_key)
                except Exception as e:
                    logging.error("Error processing PDF", exc_info=True)
                    upsert_dynamoDB_status(partition_key, sort_key, object_key, "Failed", str(e))

                delete_message_from_queue(sqs_queue_url, message)
                logging.info(f"{sort_key} - Process Completed")

    except KeyboardInterrupt:
        logging.info("Process terminated by user")
    except Exception as e:
        logging.critical("Fatal error in main loop", exc_info=True)

if __name__ == '__main__':
    main()

##################################################################################################

# <<<<<<< Modified: Separate AWS Client Initialization for Debugging >>>>>>>

try:
    s3 = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name=REGION, verify=False)
    logging.info("Initialized S3 client")
except Exception as e:
    logging.critical("Failed to initialize S3 client", exc_info=True)
    raise SystemExit("Terminating due to S3 client initialization failure.")

try:
    s3_prod = boto3.client('s3', aws_access_key_id=access_key_prod, aws_secret_access_key=secret_key_prod, region_name=REGION, verify=False)
    logging.info("Initialized S3 (Prod) client")
except Exception as e:
    logging.critical("Failed to initialize S3 Prod client", exc_info=True)
    raise SystemExit("Terminating due to S3 Prod client initialization failure.")

try:
    textract = boto3.client('textract', aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name=REGION, verify=False)
    logging.info("Initialized Textract client")
except Exception as e:
    logging.critical("Failed to initialize Textract client", exc_info=True)
    raise SystemExit("Terminating due to Textract client initialization failure.")

try:
    dynamodb = boto3.resource('dynamodb', aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name=REGION, verify=False)
    logging.info("Initialized DynamoDB resource")
except Exception as e:
    logging.critical("Failed to initialize DynamoDB resource", exc_info=True)
    raise SystemExit("Terminating due to DynamoDB resource initialization failure.")

try:
    sqs = boto3.client('sqs', aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name=REGION, verify=False)
    logging.info("Initialized SQS client")
except Exception as e:
    logging.critical("Failed to initialize SQS client", exc_info=True)
    raise SystemExit("Terminating due to SQS client initialization failure.")

try:
    bedrock = boto3.client('bedrock-runtime', aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name=REGION, verify=False)
    logging.info("Initialized Bedrock client")
except Exception as e:
    logging.critical("Failed to initialize Bedrock client", exc_info=True)
    raise SystemExit("Terminating due to Bedrock client initialization failure.")
