You need to modify this lambda function:


import os
import boto3
import csv
from io import StringIO
from pathlib import Path

# Object Initializations
s3 = boto3.client('s3')
sqs = boto3.client('sqs')

def add_data_to_queue(queueUrl, object_key):
    return sqs.send_message(
        QueueUrl = queueUrl,
        MessageBody = object_key
    )

def read_csv_from_s3(bucket_name, object_key):
    response = s3.get_object(Bucket=bucket_name, Key=object_key)
    csv_content = response['Body'].read().decode('utf-8')
    csv_reader = csv.reader(StringIO(csv_content))
    return csv_reader

def lambda_handler(event, context):
    # Variable Initializations
    bucket_name = os.environ.get('CSV_BUCKET_NAME')
    object_key = os.environ.get('CSV_OBJECT_KEY')
    queueUrl = os.environ.get('SQS_QUEUE_URL')

    file_extension = Path(object_key).suffix.lower()
    if file_extension != '.csv':
        return "provided file is not csv"

    csv_reader = read_csv_from_s3(bucket_name, object_key)
    for row in csv_reader:
        add_data_to_queue(queueUrl, row[7])

    return "Done"


With this environmental variables:

Key                                      Value
CSV_BUCKET_NAME               pii-redaction-code

CSV_OBJECT_KEY                Dev/files.csv

SQS_QUEUE_URL                 https://sqs.us-east-1.amazonaws.com/160885272637/pii-redaction-rawpdfs


####################################
import os
import boto3
import re

# Initialize AWS clients
s3 = boto3.client('s3')
sqs = boto3.client('sqs')

# Compile the regex pattern once
FILE_NAME_PATTERN = re.compile(r'^T_\d+\.PDF$', re.IGNORECASE)

def list_matching_pdf_files(bucket_name, prefix=''):
    paginator = s3.get_paginator('list_objects_v2')
    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)

    matched_files = []
    for page in page_iterator:
        for obj in page.get('Contents', []):
            key = obj['Key']
            filename = key.split('/')[-1]  # get only the filename, not the full path
            if FILE_NAME_PATTERN.match(filename):
                matched_files.append(key)
    return matched_files

def send_to_sqs(queue_url, file_name):
    return sqs.send_message(
        QueueUrl=queue_url,
        MessageBody=file_name
    )

def lambda_handler(event, context):
    # Get environment variables
    bucket_name = os.environ['PDF_BUCKET_NAME']
    prefix = os.environ.get('PDF_OBJECT_PREFIX', '')
    queue_url = os.environ['SQS_QUEUE_URL']

    # List matching PDF files
    pdf_files = list_matching_pdf_files(bucket_name, prefix)

    # Send each file name to SQS
    for pdf_file in pdf_files:
        send_to_sqs(queue_url, pdf_file)

    return {
        'statusCode': 200,
        'body': f'{len(pdf_files)} matching PDF file names sent to SQS.'
    }


#################################### lambda from s3 directly:

import os
import boto3

s3 = boto3.client('s3')
sqs = boto3.client('sqs')

def list_pdf_files(s3_client, bucket_name, prefix=''):
    paginator = s3_client.get_paginator('list_objects_v2')
    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)
    pdf_files = []
    for page in page_iterator:
        for obj in page.get('Contents', []):
            key = obj['Key']
            if key.lower().endswith('.pdf'):
                pdf_files.append(key)
    return pdf_files

def add_data_to_queue(queueUrl, object_key):
    return sqs.send_message(
        QueueUrl=queueUrl,
        MessageBody=object_key
    )

def lambda_handler(event, context):
    bucket_name = os.environ['PDF_BUCKET_NAME']  # <-- UNCOMMENTED
    queue_url = os.environ['SQS_QUEUE_URL']      # <-- UNCOMMENTED
    prefix = os.environ.get('PDF_OBJECT_PREFIX', '')  # <-- UNCOMMENTED

    pdf_files = list_pdf_files(s3, bucket_name, prefix)
    for pdf_file in pdf_files:
        add_data_to_queue(queue_url, pdf_file)
    return {"status": "Done", "files_sent": len(pdf_files)}



env:
BUCKET_NAME   -------
SQS_QUEUE_URL  -----

#####################

S3: Name, Type, Last modified, Size, Storage class

