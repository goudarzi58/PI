You need to modify this lambda function:


import os
import boto3
import csv
from io import StringIO
from pathlib import Path

# Object Initializations
s3 = boto3.client('s3')
sqs = boto3.client('sqs')

def add_data_to_queue(queueUrl, object_key):
    return sqs.send_message(
        QueueUrl = queueUrl,
        MessageBody = object_key
    )

def read_csv_from_s3(bucket_name, object_key):
    response = s3.get_object(Bucket=bucket_name, Key=object_key)
    csv_content = response['Body'].read().decode('utf-8')
    csv_reader = csv.reader(StringIO(csv_content))
    return csv_reader

def lambda_handler(event, context):
    # Variable Initializations
    bucket_name = os.environ.get('CSV_BUCKET_NAME')
    object_key = os.environ.get('CSV_OBJECT_KEY')
    queueUrl = os.environ.get('SQS_QUEUE_URL')

    file_extension = Path(object_key).suffix.lower()
    if file_extension != '.csv':
        return "provided file is not csv"

    csv_reader = read_csv_from_s3(bucket_name, object_key)
    for row in csv_reader:
        add_data_to_queue(queueUrl, row[7])

    return "Done"


With this environmental variables:

Key                                      Value
CSV_BUCKET_NAME               pii-redaction-code

CSV_OBJECT_KEY                Dev/files.csv

SQS_QUEUE_URL                 https://sqs.us-east-1.amazonaws.com/160885272637/pii-redaction-rawpdfs


####################################
import os
import boto3
import re

# Initialize AWS clients
s3 = boto3.client('s3')
sqs = boto3.client('sqs')

# Compile the regex pattern once
FILE_NAME_PATTERN = re.compile(r'^T_\d+\.PDF$', re.IGNORECASE)

def list_matching_pdf_files(bucket_name, prefix=''):
    paginator = s3.get_paginator('list_objects_v2')
    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)

    matched_files = []
    for page in page_iterator:
        for obj in page.get('Contents', []):
            key = obj['Key']
            filename = key.split('/')[-1]  # get only the filename, not the full path
            if FILE_NAME_PATTERN.match(filename):
                matched_files.append(key)
    return matched_files

def send_to_sqs(queue_url, file_name):
    return sqs.send_message(
        QueueUrl=queue_url,
        MessageBody=file_name
    )

def lambda_handler(event, context):
    # Get environment variables
    bucket_name = os.environ['PDF_BUCKET_NAME']
    prefix = os.environ.get('PDF_OBJECT_PREFIX', '')
    queue_url = os.environ['SQS_QUEUE_URL']

    # List matching PDF files
    pdf_files = list_matching_pdf_files(bucket_name, prefix)

    # Send each file name to SQS
    for pdf_file in pdf_files:
        send_to_sqs(queue_url, pdf_file)

    return {
        'statusCode': 200,
        'body': f'{len(pdf_files)} matching PDF file names sent to SQS.'
    }


#################################### lambda from s3 directly:

import os
import boto3

s3 = boto3.client('s3')
sqs = boto3.client('sqs')

def list_pdf_files(s3_client, bucket_name, prefix=''):
    paginator = s3_client.get_paginator('list_objects_v2')
    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)
    pdf_files = []
    for page in page_iterator:
        for obj in page.get('Contents', []):
            key = obj['Key']
            if key.lower().endswith('.pdf'):
                pdf_files.append(key)
    return pdf_files

def add_data_to_queue(queueUrl, object_key):
    return sqs.send_message(
        QueueUrl=queueUrl,
        MessageBody=object_key
    )

def lambda_handler(event, context):
    bucket_name = os.environ['PDF_BUCKET_NAME']  # <-- UNCOMMENTED
    queue_url = os.environ['SQS_QUEUE_URL']      # <-- UNCOMMENTED
    prefix = os.environ.get('PDF_OBJECT_PREFIX', '')  # <-- UNCOMMENTED

    pdf_files = list_pdf_files(s3, bucket_name, prefix)
    for pdf_file in pdf_files:
        add_data_to_queue(queue_url, pdf_file)
    return {"status": "Done", "files_sent": len(pdf_files)}



env:
BUCKET_NAME   -------
SQS_QUEUE_URL  -----

#####################

S3: Name, Type, Last modified, Size, Storage class

#####################
This (yellow highlighted) is the bucket details and file name should look for.
https://sos-appian-prod-ucc.s3.amazonaws.com/T000591995700024V0000R000150004857545P00092D000591995700024UCC_BEST_OriginalFiling_591995700024.PDF
--There should be around 4.15M. I think VC mentioned newest to  oldest order. 

Don’t look for
https://sos-appian-prod-ucc.s3.amazonaws.com/T000565949930012V0000R000140027977815P00102D000565949930012UCC_BEST_OriginalFilingNonRedacted_565949930012.PDF
https://sos-appian-prod-ucc.s3.amazonaws.com/T000633769790001V0000R000150031785040P00093D000633769790001UCC_BEST_Acknowledgment_633769790001_17957502.PDF

######################################
The modified Lambda function based on your prototype and requirements. It:

Filters files by name (UCC_BEST_OriginalFiling)

Ignores unwanted patterns

Sorts by LastModified descending (newest to oldest upload order)

Assigns file_ids

Checks if the SQS queue is empty

Sends messages in chunks (e.g., 50,000)

Avoids duplication (if integrated with external deduplication tracking)
######################################

import os
import boto3
from botocore.exceptions import ClientError

# AWS Clients
s3 = boto3.client('s3')
sqs = boto3.client('sqs')

# Constants
CHUNK_SIZE = 50000
INCLUDE_PATTERN = 'UCC_BEST_OriginalFiling'
EXCLUDE_PATTERNS = ['BEST_OriginalFilingNonRedacted', 'UCC_BEST_Acknowledgment']

def list_filtered_pdf_files(bucket_name, prefix=''):
    paginator = s3.get_paginator('list_objects_v2')
    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)

    filtered_files = []
    for page in page_iterator:
        for obj in page.get('Contents', []):
            key = obj['Key']
            if key.lower().endswith('.pdf') and INCLUDE_PATTERN in key:
                if all(excl not in key for excl in EXCLUDE_PATTERNS):
                    filtered_files.append({
                        "Key": key,
                        "LastModified": obj["LastModified"]
                    })
    return filtered_files

def is_sqs_queue_empty(queue_url):
    attrs = sqs.get_queue_attributes(
        QueueUrl=queue_url,
        AttributeNames=['ApproximateNumberOfMessages', 'ApproximateNumberOfMessagesNotVisible']
    )
    visible = int(attrs['Attributes'].get('ApproximateNumberOfMessages', 0))
    not_visible = int(attrs['Attributes'].get('ApproximateNumberOfMessagesNotVisible', 0))
    return (visible + not_visible) == 0

def send_batch_to_sqs(queue_url, files_batch, start_index):
    for i, file in enumerate(files_batch):
        message_body = {
            "file_id": start_index + i + 1,
            "s3_key": file["Key"],
            "last_modified": file["LastModified"].isoformat()
        }
        sqs.send_message(
            QueueUrl=queue_url,
            MessageBody=str(message_body)
        )

def lambda_handler(event, context):
    # --- ENVIRONMENT VARIABLES ---
    bucket_name = os.environ['PDF_BUCKET_NAME']
    queue_url = os.environ['SQS_QUEUE_URL']
    prefix = os.environ.get('PDF_OBJECT_PREFIX', '')

    # Step 1: Check if SQS queue is empty
    if not is_sqs_queue_empty(queue_url):
        return {"status": "SQS not empty, skipping this run."}

    # Step 2: List and filter files
    files = list_filtered_pdf_files(bucket_name, prefix)

    # Step 3: Sort files (newest first, based on LastModified)
    sorted_files = sorted(files, key=lambda x: x["LastModified"], reverse=True)

    # Step 4: Chunk and send to SQS
    total_sent = 0
    for i in range(0, len(sorted_files), CHUNK_SIZE):
        batch = sorted_files[i:i + CHUNK_SIZE]
        send_batch_to_sqs(queue_url, batch, start_index=i)
        total_sent += len(batch)
        break  # Send only one batch per run

    return {
        "status": "Batch sent",
        "files_sent": total_sent
    }
###############################################
{
  "MessageId": "PLACEHOLDER_ID_2",
  "ReceiptHandle": "PLACEHOLDER_RECEIPT_2",
  "MD5OfBody": "PLACEHOLDER_MD5_2",
  "Body": "T000046553550002V0000R000040046646060P00092D000046553550002UCC_BEST_OriginalFiling_46553550002.PDF",
  "Attributes": {}
}
#############################################
{
  'file_id': 1, 
  's3_key': 'T000000000449534V0000R000007400006611P00102D000000000449534UCC_BEST_OriginalFiling_449534.PDF', 'last_modified': '2025-06-20T17:35:28+00:00'
}

#################################################

import os
import boto3
from botocore.exceptions import ClientError

# AWS Clients
s3 = boto3.client('s3')
sqs = boto3.client('sqs')

# Constants
CHUNK_SIZE = 50000
INCLUDE_PATTERN = 'UCC_BEST_OriginalFiling'
EXCLUDE_PATTERNS = ['BEST_OriginalFilingNonRedacted', 'UCC_BEST_Acknowledgment']

def list_filtered_pdf_files(bucket_name, prefix=''):
    paginator = s3.get_paginator('list_objects_v2')
    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)

    filtered_files = []
    for page in page_iterator:
        for obj in page.get('Contents', []):
            key = obj['Key']
            if key.lower().endswith('.pdf') and INCLUDE_PATTERN in key:
                if all(excl not in key for excl in EXCLUDE_PATTERNS):
                    filtered_files.append({
                        "Key": key,
                        "LastModified": obj["LastModified"]
                    })
    return filtered_files

def is_sqs_queue_empty(queue_url):
    attrs = sqs.get_queue_attributes(
        QueueUrl=queue_url,
        AttributeNames=['ApproximateNumberOfMessages', 'ApproximateNumberOfMessagesNotVisible']
    )
    visible = int(attrs['Attributes'].get('ApproximateNumberOfMessages', 0))
    not_visible = int(attrs['Attributes'].get('ApproximateNumberOfMessagesNotVisible', 0))
    return (visible + not_visible) == 0

def send_batch_to_sqs(queue_url, files_batch, start_index):
    for i, file in enumerate(files_batch):
        s3_key = file["Key"]
        file_name = s3_key.split('/')[-1]
        file_id = start_index + i + 1

        message_payload = {
            "file_id": file_id,
            "MessageId": f"PLACEHOLDER_ID_{file_id}",
            "ReceiptHandle": f"PLACEHOLDER_RECEIPT_{file_id}",
            "MD5OfBody": f"PLACEHOLDER_MD5_{file_id}",
            "Body": file_name,
            "Attributes": {}
        }

        sqs.send_message(
            QueueUrl=queue_url,
            MessageBody=str(message_payload)  # sent as string
        )

def lambda_handler(event, context):
    # --- ENVIRONMENT VARIABLES ---
    bucket_name = os.environ['PDF_BUCKET_NAME']
    queue_url = os.environ['SQS_QUEUE_URL']
    prefix = os.environ.get('PDF_OBJECT_PREFIX', '')

    # Step 1: Check if SQS queue is empty
    if not is_sqs_queue_empty(queue_url):
        return {"status": "SQS not empty, skipping this run."}

    # Step 2: List and filter files
    files = list_filtered_pdf_files(bucket_name, prefix)

    # Step 3: Sort files (newest first, based on LastModified)
    sorted_files = sorted(files, key=lambda x: x["LastModified"], reverse=True)

    # Step 4: Chunk and send to SQS
    total_sent = 0
    for i in range(0, len(sorted_files), CHUNK_SIZE):
        batch = sorted_files[i:i + CHUNK_SIZE]
        send_batch_to_sqs(queue_url, batch, start_index=i)
        total_sent += len(batch)
        break  # Send only one batch per run

    return {
        "status": "Batch sent",
        "files_sent": total_sent
    }
############################################################
 Logic Implemented
Send messages in batches of 10

Continue sending batches until reaching CHUNK_SIZE (e.g., 50,000)

Then pause and check every 10 minutes to see if the SQS queue is empty

Once empty, start sending the next CHUNK_SIZE

Ensures each file is processed only once per Lambda run
############################################################
Configuration Parameters
CHUNK_SIZE: Number of files to send before pausing

WAIT_TIME_SEC: How long to wait (default: 600 sec = 10 min)
############################################################

import os
import time
import boto3

# Initialize AWS clients
s3 = boto3.client('s3')
sqs = boto3.client('sqs')

# Configuration constants
CHUNK_SIZE = 50000               # Number of messages to send in one batch cycle
BATCH_SIZE = 10                  # Number of messages to send per SQS batch (max allowed: 10)
WAIT_TIME_SEC = 600              # Wait time between chunk cycles if queue is not empty (10 minutes)

# Filtering criteria
INCLUDE_PATTERN = 'UCC_BEST_OriginalFiling'
EXCLUDE_PATTERNS = ['BEST_OriginalFilingNonRedacted', 'UCC_BEST_Acknowledgment']


def list_filtered_pdf_files(bucket_name, prefix=''):
    """
    List all PDF files from S3 that include the target pattern and exclude others.
    """
    paginator = s3.get_paginator('list_objects_v2')
    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)

    filtered_files = []
    for page in page_iterator:
        for obj in page.get('Contents', []):
            key = obj['Key']
            if key.lower().endswith('.pdf') and INCLUDE_PATTERN in key:
                if all(excl not in key for excl in EXCLUDE_PATTERNS):
                    filtered_files.append({
                        "Key": key,
                        "LastModified": obj["LastModified"]
                    })
    return filtered_files


def is_sqs_queue_empty(queue_url):
    """
    Check if the SQS queue is empty by checking both visible and invisible messages.
    """
    attrs = sqs.get_queue_attributes(
        QueueUrl=queue_url,
        AttributeNames=['ApproximateNumberOfMessages', 'ApproximateNumberOfMessagesNotVisible']
    )
    visible = int(attrs['Attributes'].get('ApproximateNumberOfMessages', 0))
    not_visible = int(attrs['Attributes'].get('ApproximateNumberOfMessagesNotVisible', 0))
    return (visible + not_visible) == 0


def send_in_batches(files, queue_url):
    """
    Send filtered files to SQS in groups of BATCH_SIZE.
    After each CHUNK_SIZE messages, wait until the queue is empty before resuming.
    """
    sent_total = 0       # Tracks number of messages sent in the current CHUNK
    batch_number = 0     # Increments after each CHUNK_SIZE

    for i in range(0, len(files), BATCH_SIZE):
        # If CHUNK_SIZE reached, wait until SQS queue is empty
        if sent_total >= CHUNK_SIZE:
            print(f"✅ Sent {CHUNK_SIZE} messages. Waiting for queue to empty...")
            while not is_sqs_queue_empty(queue_url):
                print(f"⏳ Queue not empty. Waiting {WAIT_TIME_SEC // 60} minutes...")
                time.sleep(WAIT_TIME_SEC)
            print("✅ Queue is now empty. Resuming...")
            sent_total = 0
            batch_number += 1

        # Select the next batch of files
        batch = files[i:i + BATCH_SIZE]

        # Construct entries for send_message_batch
        entries = []
        for j, file in enumerate(batch):
            file_id = (batch_number * CHUNK_SIZE) + sent_total + j + 1
            file_name = file["Key"].split('/')[-1]

            msg_body = {
                "file_id": file_id,
                "MessageId": f"PLACEHOLDER_ID_{file_id}",
                "ReceiptHandle": f"PLACEHOLDER_RECEIPT_{file_id}",
                "MD5OfBody": f"PLACEHOLDER_MD5_{file_id}",
                "Body": file_name,
                "Attributes": {}
            }

            entries.append({
                'Id': str(j),  # Must be unique within batch
                'MessageBody': str(msg_body)
            })

        # Send batch to SQS
        sqs.send_message_batch(QueueUrl=queue_url, Entries=entries)
        sent_total += len(batch)

    return (batch_number * CHUNK_SIZE) + sent_total


def lambda_handler(event, context):
    """
    Lambda entry point: filter S3 files, sort by LastModified,
    and push them to SQS in controlled, chunked batches.
    """
    bucket_name = os.environ['PDF_BUCKET_NAME']
    queue_url = os.environ['SQS_QUEUE_URL']
    prefix = os.environ.get('PDF_OBJECT_PREFIX', '')

    # Step 1: Load and filter files
    all_files = list_filtered_pdf_files(bucket_name, prefix)

    # Step 2: Sort newest to oldest by upload timestamp
    sorted_files = sorted(all_files, key=lambda x: x["LastModified"], reverse=True)

    # Step 3: Send to SQS in batches
    total_sent = send_in_batches(sorted_files, queue_url)

    return {
        "status": "Done",
        "total_files_sent": total_sent
    }
