You need to modify this lambda function:


import os
import boto3
import csv
from io import StringIO
from pathlib import Path

# Object Initializations
s3 = boto3.client('s3')
sqs = boto3.client('sqs')

def add_data_to_queue(queueUrl, object_key):
    return sqs.send_message(
        QueueUrl = queueUrl,
        MessageBody = object_key
    )

def read_csv_from_s3(bucket_name, object_key):
    response = s3.get_object(Bucket=bucket_name, Key=object_key)
    csv_content = response['Body'].read().decode('utf-8')
    csv_reader = csv.reader(StringIO(csv_content))
    return csv_reader

def lambda_handler(event, context):
    # Variable Initializations
    bucket_name = os.environ.get('CSV_BUCKET_NAME')
    object_key = os.environ.get('CSV_OBJECT_KEY')
    queueUrl = os.environ.get('SQS_QUEUE_URL')

    file_extension = Path(object_key).suffix.lower()
    if file_extension != '.csv':
        return "provided file is not csv"

    csv_reader = read_csv_from_s3(bucket_name, object_key)
    for row in csv_reader:
        add_data_to_queue(queueUrl, row[7])

    return "Done"


With this environmental variables:

Key                                      Value
CSV_BUCKET_NAME               pii-redaction-code

CSV_OBJECT_KEY                Dev/files.csv

SQS_QUEUE_URL                 https://sqs.us-east-1.amazonaws.com/160885272637/pii-redaction-rawpdfs


####################################
import os
import boto3
import re

# Initialize AWS clients
s3 = boto3.client('s3')
sqs = boto3.client('sqs')

# Compile the regex pattern once
FILE_NAME_PATTERN = re.compile(r'^T_\d+\.PDF$', re.IGNORECASE)

def list_matching_pdf_files(bucket_name, prefix=''):
    paginator = s3.get_paginator('list_objects_v2')
    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)

    matched_files = []
    for page in page_iterator:
        for obj in page.get('Contents', []):
            key = obj['Key']
            filename = key.split('/')[-1]  # get only the filename, not the full path
            if FILE_NAME_PATTERN.match(filename):
                matched_files.append(key)
    return matched_files

def send_to_sqs(queue_url, file_name):
    return sqs.send_message(
        QueueUrl=queue_url,
        MessageBody=file_name
    )

def lambda_handler(event, context):
    # Get environment variables
    bucket_name = os.environ['PDF_BUCKET_NAME']
    prefix = os.environ.get('PDF_OBJECT_PREFIX', '')
    queue_url = os.environ['SQS_QUEUE_URL']

    # List matching PDF files
    pdf_files = list_matching_pdf_files(bucket_name, prefix)

    # Send each file name to SQS
    for pdf_file in pdf_files:
        send_to_sqs(queue_url, pdf_file)

    return {
        'statusCode': 200,
        'body': f'{len(pdf_files)} matching PDF file names sent to SQS.'
    }


#################################### lambda from s3 directly:

import os
import boto3

s3 = boto3.client('s3')
sqs = boto3.client('sqs')

def list_pdf_files(s3_client, bucket_name, prefix=''):
    paginator = s3_client.get_paginator('list_objects_v2')
    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)
    pdf_files = []
    for page in page_iterator:
        for obj in page.get('Contents', []):
            key = obj['Key']
            if key.lower().endswith('.pdf'):
                pdf_files.append(key)
    return pdf_files

def add_data_to_queue(queueUrl, object_key):
    return sqs.send_message(
        QueueUrl=queueUrl,
        MessageBody=object_key
    )

def lambda_handler(event, context):
    bucket_name = os.environ['PDF_BUCKET_NAME']  # <-- UNCOMMENTED
    queue_url = os.environ['SQS_QUEUE_URL']      # <-- UNCOMMENTED
    prefix = os.environ.get('PDF_OBJECT_PREFIX', '')  # <-- UNCOMMENTED

    pdf_files = list_pdf_files(s3, bucket_name, prefix)
    for pdf_file in pdf_files:
        add_data_to_queue(queue_url, pdf_file)
    return {"status": "Done", "files_sent": len(pdf_files)}



env:
BUCKET_NAME   -------
SQS_QUEUE_URL  -----

#####################

S3: Name, Type, Last modified, Size, Storage class

#####################
This (yellow highlighted) is the bucket details and file name should look for.
https://sos-appian-prod-ucc.s3.amazonaws.com/T000591995700024V0000R000150004857545P00092D000591995700024UCC_BEST_OriginalFiling_591995700024.PDF
--There should be around 4.15M. I think VC mentioned newest to  oldest order. 

Don’t look for
https://sos-appian-prod-ucc.s3.amazonaws.com/T000565949930012V0000R000140027977815P00102D000565949930012UCC_BEST_OriginalFilingNonRedacted_565949930012.PDF
https://sos-appian-prod-ucc.s3.amazonaws.com/T000633769790001V0000R000150031785040P00093D000633769790001UCC_BEST_Acknowledgment_633769790001_17957502.PDF

######################################
The modified Lambda function based on your prototype and requirements. It:

Filters files by name (UCC_BEST_OriginalFiling)

Ignores unwanted patterns

Sorts by LastModified descending (newest to oldest upload order)

Assigns file_ids

Checks if the SQS queue is empty

Sends messages in chunks (e.g., 50,000)

Avoids duplication (if integrated with external deduplication tracking)
######################################

import os
import boto3
from botocore.exceptions import ClientError

# AWS Clients
s3 = boto3.client('s3')
sqs = boto3.client('sqs')

# Constants
CHUNK_SIZE = 50000
INCLUDE_PATTERN = 'UCC_BEST_OriginalFiling'
EXCLUDE_PATTERNS = ['BEST_OriginalFilingNonRedacted', 'UCC_BEST_Acknowledgment']

def list_filtered_pdf_files(bucket_name, prefix=''):
    paginator = s3.get_paginator('list_objects_v2')
    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)

    filtered_files = []
    for page in page_iterator:
        for obj in page.get('Contents', []):
            key = obj['Key']
            if key.lower().endswith('.pdf') and INCLUDE_PATTERN in key:
                if all(excl not in key for excl in EXCLUDE_PATTERNS):
                    filtered_files.append({
                        "Key": key,
                        "LastModified": obj["LastModified"]
                    })
    return filtered_files

def is_sqs_queue_empty(queue_url):
    attrs = sqs.get_queue_attributes(
        QueueUrl=queue_url,
        AttributeNames=['ApproximateNumberOfMessages', 'ApproximateNumberOfMessagesNotVisible']
    )
    visible = int(attrs['Attributes'].get('ApproximateNumberOfMessages', 0))
    not_visible = int(attrs['Attributes'].get('ApproximateNumberOfMessagesNotVisible', 0))
    return (visible + not_visible) == 0

def send_batch_to_sqs(queue_url, files_batch, start_index):
    for i, file in enumerate(files_batch):
        message_body = {
            "file_id": start_index + i + 1,
            "s3_key": file["Key"],
            "last_modified": file["LastModified"].isoformat()
        }
        sqs.send_message(
            QueueUrl=queue_url,
            MessageBody=str(message_body)
        )

def lambda_handler(event, context):
    # --- ENVIRONMENT VARIABLES ---
    bucket_name = os.environ['PDF_BUCKET_NAME']
    queue_url = os.environ['SQS_QUEUE_URL']
    prefix = os.environ.get('PDF_OBJECT_PREFIX', '')

    # Step 1: Check if SQS queue is empty
    if not is_sqs_queue_empty(queue_url):
        return {"status": "SQS not empty, skipping this run."}

    # Step 2: List and filter files
    files = list_filtered_pdf_files(bucket_name, prefix)

    # Step 3: Sort files (newest first, based on LastModified)
    sorted_files = sorted(files, key=lambda x: x["LastModified"], reverse=True)

    # Step 4: Chunk and send to SQS
    total_sent = 0
    for i in range(0, len(sorted_files), CHUNK_SIZE):
        batch = sorted_files[i:i + CHUNK_SIZE]
        send_batch_to_sqs(queue_url, batch, start_index=i)
        total_sent += len(batch)
        break  # Send only one batch per run

    return {
        "status": "Batch sent",
        "files_sent": total_sent
    }
###############################################
{
  "MessageId": "PLACEHOLDER_ID_2",
  "ReceiptHandle": "PLACEHOLDER_RECEIPT_2",
  "MD5OfBody": "PLACEHOLDER_MD5_2",
  "Body": "T000046553550002V0000R000040046646060P00092D000046553550002UCC_BEST_OriginalFiling_46553550002.PDF",
  "Attributes": {}
}
#############################################
{
  'file_id': 1, 
  's3_key': 'T000000000449534V0000R000007400006611P00102D000000000449534UCC_BEST_OriginalFiling_449534.PDF', 'last_modified': '2025-06-20T17:35:28+00:00'
}

#################################################

import os
import boto3
from botocore.exceptions import ClientError

# AWS Clients
s3 = boto3.client('s3')
sqs = boto3.client('sqs')

# Constants
CHUNK_SIZE = 50000
INCLUDE_PATTERN = 'UCC_BEST_OriginalFiling'
EXCLUDE_PATTERNS = ['BEST_OriginalFilingNonRedacted', 'UCC_BEST_Acknowledgment']

def list_filtered_pdf_files(bucket_name, prefix=''):
    paginator = s3.get_paginator('list_objects_v2')
    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)

    filtered_files = []
    for page in page_iterator:
        for obj in page.get('Contents', []):
            key = obj['Key']
            if key.lower().endswith('.pdf') and INCLUDE_PATTERN in key:
                if all(excl not in key for excl in EXCLUDE_PATTERNS):
                    filtered_files.append({
                        "Key": key,
                        "LastModified": obj["LastModified"]
                    })
    return filtered_files

def is_sqs_queue_empty(queue_url):
    attrs = sqs.get_queue_attributes(
        QueueUrl=queue_url,
        AttributeNames=['ApproximateNumberOfMessages', 'ApproximateNumberOfMessagesNotVisible']
    )
    visible = int(attrs['Attributes'].get('ApproximateNumberOfMessages', 0))
    not_visible = int(attrs['Attributes'].get('ApproximateNumberOfMessagesNotVisible', 0))
    return (visible + not_visible) == 0

def send_batch_to_sqs(queue_url, files_batch, start_index):
    for i, file in enumerate(files_batch):
        s3_key = file["Key"]
        file_name = s3_key.split('/')[-1]
        file_id = start_index + i + 1

        message_payload = {
            "file_id": file_id,
            "MessageId": f"PLACEHOLDER_ID_{file_id}",
            "ReceiptHandle": f"PLACEHOLDER_RECEIPT_{file_id}",
            "MD5OfBody": f"PLACEHOLDER_MD5_{file_id}",
            "Body": file_name,
            "Attributes": {}
        }

        sqs.send_message(
            QueueUrl=queue_url,
            MessageBody=str(message_payload)  # sent as string
        )

def lambda_handler(event, context):
    # --- ENVIRONMENT VARIABLES ---
    bucket_name = os.environ['PDF_BUCKET_NAME']
    queue_url = os.environ['SQS_QUEUE_URL']
    prefix = os.environ.get('PDF_OBJECT_PREFIX', '')

    # Step 1: Check if SQS queue is empty
    if not is_sqs_queue_empty(queue_url):
        return {"status": "SQS not empty, skipping this run."}

    # Step 2: List and filter files
    files = list_filtered_pdf_files(bucket_name, prefix)

    # Step 3: Sort files (newest first, based on LastModified)
    sorted_files = sorted(files, key=lambda x: x["LastModified"], reverse=True)

    # Step 4: Chunk and send to SQS
    total_sent = 0
    for i in range(0, len(sorted_files), CHUNK_SIZE):
        batch = sorted_files[i:i + CHUNK_SIZE]
        send_batch_to_sqs(queue_url, batch, start_index=i)
        total_sent += len(batch)
        break  # Send only one batch per run

    return {
        "status": "Batch sent",
        "files_sent": total_sent
    }
############################################################
 Logic Implemented
Send messages in batches of 10

Continue sending batches until reaching CHUNK_SIZE (e.g., 50,000)

Then pause and check every 10 minutes to see if the SQS queue is empty

Once empty, start sending the next CHUNK_SIZE

Ensures each file is processed only once per Lambda run
############################################################
Configuration Parameters
CHUNK_SIZE: Number of files to send before pausing

WAIT_TIME_SEC: How long to wait (default: 600 sec = 10 min)
############################################################

import os
import time
import boto3

# Initialize AWS clients
s3 = boto3.client('s3')
sqs = boto3.client('sqs')

# Configuration constants
CHUNK_SIZE = 50000               # Number of messages to send in one batch cycle
BATCH_SIZE = 10                  # Number of messages to send per SQS batch (max allowed: 10)
WAIT_TIME_SEC = 600              # Wait time between chunk cycles if queue is not empty (10 minutes)

# Filtering criteria
INCLUDE_PATTERN = 'UCC_BEST_OriginalFiling'
EXCLUDE_PATTERNS = ['BEST_OriginalFilingNonRedacted', 'UCC_BEST_Acknowledgment']


def list_filtered_pdf_files(bucket_name, prefix=''):
    """
    List all PDF files from S3 that include the target pattern and exclude others.
    """
    paginator = s3.get_paginator('list_objects_v2')
    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)

    filtered_files = []
    for page in page_iterator:
        for obj in page.get('Contents', []):
            key = obj['Key']
            if key.lower().endswith('.pdf') and INCLUDE_PATTERN in key:
                if all(excl not in key for excl in EXCLUDE_PATTERNS):
                    filtered_files.append({
                        "Key": key,
                        "LastModified": obj["LastModified"]
                    })
    return filtered_files


def is_sqs_queue_empty(queue_url):
    """
    Check if the SQS queue is empty by checking both visible and invisible messages.
    """
    attrs = sqs.get_queue_attributes(
        QueueUrl=queue_url,
        AttributeNames=['ApproximateNumberOfMessages', 'ApproximateNumberOfMessagesNotVisible']
    )
    visible = int(attrs['Attributes'].get('ApproximateNumberOfMessages', 0))
    not_visible = int(attrs['Attributes'].get('ApproximateNumberOfMessagesNotVisible', 0))
    return (visible + not_visible) == 0


def send_in_batches(files, queue_url):
    """
    Send filtered files to SQS in groups of BATCH_SIZE.
    After each CHUNK_SIZE messages, wait until the queue is empty before resuming.
    """
    sent_total = 0       # Tracks number of messages sent in the current CHUNK
    batch_number = 0     # Increments after each CHUNK_SIZE

    for i in range(0, len(files), BATCH_SIZE):
        # If CHUNK_SIZE reached, wait until SQS queue is empty
        if sent_total >= CHUNK_SIZE:
            print(f"✅ Sent {CHUNK_SIZE} messages. Waiting for queue to empty...")
            while not is_sqs_queue_empty(queue_url):
                print(f"⏳ Queue not empty. Waiting {WAIT_TIME_SEC // 60} minutes...")
                time.sleep(WAIT_TIME_SEC)
            print("✅ Queue is now empty. Resuming...")
            sent_total = 0
            batch_number += 1

        # Select the next batch of files
        batch = files[i:i + BATCH_SIZE]

        # Construct entries for send_message_batch
        entries = []
        for j, file in enumerate(batch):
            file_id = (batch_number * CHUNK_SIZE) + sent_total + j + 1
            file_name = file["Key"].split('/')[-1]

            msg_body = {
                "file_id": file_id,
                "MessageId": f"PLACEHOLDER_ID_{file_id}",
                "ReceiptHandle": f"PLACEHOLDER_RECEIPT_{file_id}",
                "MD5OfBody": f"PLACEHOLDER_MD5_{file_id}",
                "Body": file_name,
                "Attributes": {}
            }

            entries.append({
                'Id': str(j),  # Must be unique within batch
                'MessageBody': str(msg_body)
            })

        # Send batch to SQS
        sqs.send_message_batch(QueueUrl=queue_url, Entries=entries)
        sent_total += len(batch)

    return (batch_number * CHUNK_SIZE) + sent_total


def lambda_handler(event, context):
    """
    Lambda entry point: filter S3 files, sort by LastModified,
    and push them to SQS in controlled, chunked batches.
    """
    bucket_name = os.environ['PDF_BUCKET_NAME']
    queue_url = os.environ['SQS_QUEUE_URL']
    prefix = os.environ.get('PDF_OBJECT_PREFIX', '')

    # Step 1: Load and filter files
    all_files = list_filtered_pdf_files(bucket_name, prefix)

    # Step 2: Sort newest to oldest by upload timestamp
    sorted_files = sorted(all_files, key=lambda x: x["LastModified"], reverse=True)

    # Step 3: Send to SQS in batches
    total_sent = send_in_batches(sorted_files, queue_url)

    return {
        "status": "Done",
        "total_files_sent": total_sent
    }

############################################
import os
import time
import boto3

# AWS Clients
s3 = boto3.client('s3')
sqs = boto3.client('sqs')

# Configuration Constants
START_FILE_ID = 20000           # File index to start from (inclusive)
END_FILE_ID = 50000             # File index to stop at (inclusive)
CHUNK_SIZE = 10000              # Max messages per chunk (Lambda-safe)
BATCH_SIZE = 10                 # Max messages per send_message_batch
WAIT_TIME_SEC = 600             # Wait between chunks when SQS is not empty

# File filters
INCLUDE_PATTERN = 'UCC_BEST_OriginalFiling'
EXCLUDE_PATTERNS = ['BEST_OriginalFilingNonRedacted', 'UCC_BEST_Acknowledgment']


def list_filtered_pdf_files(bucket_name, prefix=''):
    """
    List and filter PDF files in S3 by filename pattern.
    """
    paginator = s3.get_paginator('list_objects_v2')
    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)

    filtered_files = []
    for page in page_iterator:
        for obj in page.get('Contents', []):
            key = obj['Key']
            if key.lower().endswith('.pdf') and INCLUDE_PATTERN in key:
                if all(excl not in key for excl in EXCLUDE_PATTERNS):
                    filtered_files.append({
                        "Key": key,
                        "LastModified": obj["LastModified"]
                    })
    return filtered_files


def is_sqs_queue_empty(queue_url):
    """
    Check if SQS queue is empty.
    """
    attrs = sqs.get_queue_attributes(
        QueueUrl=queue_url,
        AttributeNames=['ApproximateNumberOfMessages', 'ApproximateNumberOfMessagesNotVisible']
    )
    visible = int(attrs['Attributes'].get('ApproximateNumberOfMessages', 0))
    not_visible = int(attrs['Attributes'].get('ApproximateNumberOfMessagesNotVisible', 0))
    return (visible + not_visible) == 0


def send_chunk(files, queue_url, start_index, chunk_size):
    """
    Send a CHUNK_SIZE subset of files in batches of BATCH_SIZE starting from start_index.
    Returns number of files actually sent.
    """
    sent_count = 0
    total_files = len(files)

    for i in range(start_index, min(total_files, start_index + chunk_size), BATCH_SIZE):
        if sent_count >= chunk_size:
            break

        batch = []
        for j in range(BATCH_SIZE):
            current_index = i + j
            if current_index >= total_files or sent_count >= chunk_size:
                break

            file = files[current_index]
            file_id = current_index + 1  # 1-based file_id
            file_name = file["Key"].split('/')[-1]

            msg_body = {
                "file_id": file_id,
                "MessageId": f"PLACEHOLDER_ID_{file_id}",
                "ReceiptHandle": f"PLACEHOLDER_RECEIPT_{file_id}",
                "MD5OfBody": f"PLACEHOLDER_MD5_{file_id}",
                "Body": file_name,
                "Attributes": {}
            }

            batch.append({
                'Id': str(j),
                'MessageBody': str(msg_body)
            })

            sent_count += 1

        if batch:
            sqs.send_message_batch(QueueUrl=queue_url, Entries=batch)

    return sent_count


def lambda_handler(event, context):
    """
    Lambda entry point:
    - Load files
    - Start at START_FILE_ID, stop at END_FILE_ID
    - Send in chunks (throttled by SQS emptiness)
    """
    bucket_name = os.environ['PDF_BUCKET_NAME']
    queue_url = os.environ['SQS_QUEUE_URL']
    prefix = os.environ.get('PDF_OBJECT_PREFIX', '')

    # Step 1: Load and filter all files
    all_files = list_filtered_pdf_files(bucket_name, prefix)
    sorted_files = sorted(all_files, key=lambda x: x["LastModified"], reverse=True)

    # Step 2: Enforce boundaries
    total_files = len(sorted_files)
    start_index = max(START_FILE_ID - 1, 0)
    end_index = min(END_FILE_ID, total_files)

    # Step 3: Send files in chunks
    sent_total = 0
    for i in range(start_index, end_index, CHUNK_SIZE):
        print(f" Sending chunk from file_id={i+1} to file_id={min(i + CHUNK_SIZE, end_index)}...")

        # If not the first chunk, wait for SQS to be empty
        if sent_total > 0:
            while not is_sqs_queue_empty(queue_url):
                print(f"⏳ Waiting for SQS to empty before next chunk...")
                time.sleep(WAIT_TIME_SEC)

        sent = send_chunk(sorted_files, queue_url, i, CHUNK_SIZE)
        sent_total += sent
        print(f" Sent {sent} files in this chunk.")

    return {
        "status": "Completed",
        "start_file_id": START_FILE_ID,
        "end_file_id": END_FILE_ID,
        "total_files_sent": sent_total
    }

###################################################

Lambda Function Description: Controlled S3-to-SQS Batch Messaging with Start and End File ID Control
This AWS Lambda function enables controlled, resumable dispatching of PDF file metadata from S3 to SQS in batches. It is designed for high-volume processing use cases (millions of files) and provides fine-grained control over which subset of files to process and when to pause between chunks.

Key Features
Environment-driven control: The function uses constants like START_FILE_ID, END_FILE_ID, and CHUNK_SIZE to define which portion of the file list to process during each invocation.

Skip-until-start logic: The function skips files until the current file_id reaches START_FILE_ID, then begins processing.

Fixed-size chunks: After starting, it sends messages in batches of BATCH_SIZE (10), grouped into a single chunk of up to CHUNK_SIZE messages (e.g., 10,000–20,000), suitable for AWS Lambda's 15-minute execution limit.

Queue-aware throttling: After each CHUNK_SIZE block, the function waits until the SQS queue is fully empty (is_sqs_queue_empty) before sending the next chunk. This protects downstream consumers from overload.

Stateless execution: The function does not use deduplication or state persistence. It assumes the caller controls which ranges have already been processed (e.g., through monitoring or job scheduling).

Example Use Case
With:

START_FILE_ID = 20000
END_FILE_ID = 60000
CHUNK_SIZE = 10000
The Lambda function will behave as follows:

Send file_ids 20000–29999 (10,000 messages) → then pause until SQS is empty

Send file_ids 30000–39999 → wait again

Send file_ids 40000–49999 → wait again

Send file_ids 50000–60000 → complete

Each step waits for SQS to fully drain before proceeding to the next chunk, ensuring safe, reliable batch handoff to consumers.

🛠️ Summary of Functional Logic
Step	Description
START_FILE_ID	Skips files until this file_id is reached
CHUNK_SIZE	Sends this many messages per chunk (safely fits within Lambda duration)
END_FILE_ID	Final file_id to process before exiting
is_sqs_queue_empty()	Ensures downstream consumers are ready before sending the next chunk
Stateless	No deduplication or history tracking – external coordination recommended

This function is ideal for pipeline chunking, parallelization, resumability, and safe SQS throttling during large-scale S3 data migrations or AI document processing workloads.

Let me know if you'd like a version of this as a README.md or embedded in code as a docstring.









