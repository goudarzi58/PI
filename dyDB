
{"file_name": "T001488793520001V0000R000002500300606P00092D001488793520001UCC_BEST_OriginalFiling_1488793520001.PDF", "partition_key": "2025-07-01", "sort_key": "jr_0000000a-bb48-4b1e-bf95-9a8bb366be6f", "pages": [{"1": []}]}
DynamoDB table with several columns, including:

pk (String): This appears to be the partition key, which is a date format (e.g., "2025-06-24").
file_type: This column contains strings representing the type of file, such as "Sovereign" or "Non-Sovereign".
Metadata: This column seems to contain JSON-like data, possibly with details about the file.
ObjectKey: This column contains identifiers for the objects, likely unique keys.
Status: This column indicates the current status of the file, such as "Manual Review" or "Redacted".
StatusMessage: This column provides additional information about the status, such as "File uploaded in ..." or "Successfully redacted".

To filter the content of this table based on pk, file_type, and Status, you can use Python with the AWS SDK for Python (Boto3). Below is an example code snippet that demonstrates how to query the table:
#################################################
Attributes

pk (Partition key): The value is 2025-02-12. This is likely used to uniquely identify items within the table.
jobid (Sort key): The value is jr_0056afbe-15b1-4cd9-9389-24c2ab1f3026. This is used in conjunction with the partition key to uniquely identify items.
ObjectKey: json
Status: The value is Error, suggesting there was an issue with the process or operation related to this item.
StatusMessage: The value is HITL_Review-Unknown file extension, indicating that the error might be related to a file with an unrecognized extension requiring human-in-the-loop (HITL) review.


{
  "pk": {
    "S": "2025-06-24"
  },
  "jobid": {
    "S": "jr_402edbbd-8410-42d1-8de9-da797dab8d0c"
  },
  "file_type": {
    "S": "\"Non-Sovereign File\""
  },
  "Metadata": {
    "S": "{\"file_name\": \"T000000001860101V0000R000009700215758P00102D000000001860101UCC_BEST_OriginalFilingNonRedacted_1860101.PDF\", \"partition_key\": \"2025-06-24\", \"sort_key\": \"jr_402edbbd-8410-42d1-8de9-da797dab8d0c\", \"pages\": [{\"1\": [{\"text\": \"451-52-6109\", \"Entity Type\": \"Social Security Number\", \"bbox\": [97, 612, 215, 628], \"textract_confidence_score\": 99.85, \"status\": \"redacted\"}]}]}"
  },
  "ObjectKey": {
    "S": "T000000001860101V0000R000009700215758P00102D000000001860101UCC_BEST_OriginalFilingNonRedacted_1860101.PDF"
  },
  "Status": {
    "S": "Redacted"
  },
  "StatusMessage": {
    "S": "Succesfully redacted and saved in S3"
  }
}
######################
File Name, File Name No .PDF, Redacted Text, Redacted Type, BBox, Textract Confidence Score
######################
Python Script Overview
Below is a script that:

Connects to DynamoDB using access_key / secret_key

Scans the table

Parses the ObjectKey field (which is a JSON string)

Extracts key fields

Exports the result to Excel using pandas
######################
working with DynamoDB low-level response format, where each field includes its type ("S" for string, etc.).

The ObjectKey value is a JSON string nested inside a "S" key, so you must:

Access each attribute via .get("S")

Use json.loads() on ObjectKey['S']
######################


import os
import boto3
import json
import csv

# --- Setup Paths ---
script_dir = os.path.dirname(os.path.abspath(__file__))
config_path = os.path.join(script_dir, 'config_local_run.json')
CSV_FILE = os.path.join(script_dir, 'redacted_files_output.csv')

# --- Load Config ---
with open(config_path, 'r') as f:
    config = json.load(f)

# --- Extract AWS Config ---
access_key = config['aws']['access_key']
secret_key = config['aws']['secret_key']
region = config['aws']['region']
table_name = config['services']['dynamodb']['Metadata_Table_Name']

# --- Connect to DynamoDB ---
dynamodb_client = boto3.client('dynamodb',
    aws_access_key_id=access_key,
    aws_secret_access_key=secret_key,
    region_name=region
)

# --- Parameters ---
USE_TARGET_PK = True  # Set to False to scan entire table
TARGET_PK = '2025-06-24'
TABLE_NAME = table_name

# --- Extract Metadata JSON ---
def extract_metadata(item):
    metadata_str = item.get('Metadata', {}).get('S', '{}')
    return json.loads(metadata_str)

# --- Process Item Based on File Type and Status ---
def process_item(item):
    metadata = extract_metadata(item)
    file_name = metadata.get('file_name', '') or item.get('ObjectKey', {}).get('S', '')
    file_name_no_pdf = f"{file_name.split('_')[-1]}"

    file_type = item.get('file_type', {}).get('S', '').strip('"')
    status = item.get('Status', {}).get('S', '')

    print(f"üß™ Processing file: {file_name} | type={file_type}, status={status}")

    # ‚úÖ Case 1: Sovereign File + Manual Review
    if file_type in ['Sovereign', 'Sovereign File'] and status == 'Manual Review':
        return [{
            'File Name': file_name,
            'File Name No .PDF': file_name_no_pdf,
            'Redacted Text': 'Sovereign',
            'Redacted Type': 'Sovereign',
            'BBox': 'Sovereign',
            'Textract Confidence Score': 0
        }]

    # ‚úÖ Case 2: Non-Sovereign File + Redacted
    if file_type == 'Non-Sovereign File' and status == 'Redacted':
        pages = metadata.get('pages', [])
        results = []
        seen = set()

        for page in pages:
            for _, entities in page.items():
                for ent in entities:
                    key = (ent.get('text', ''), str(ent.get('bbox', '')))
                    if key in seen:
                        continue
                    seen.add(key)
                    results.append({
                        'File Name': file_name,
                        'File Name No .PDF': file_name_no_pdf,
                        'Redacted Text': ent.get('text', ''),
                        'Redacted Type': ent.get('Entity Type', ''),
                        'BBox': ent.get('bbox', ''),
                        'Textract Confidence Score': ent.get('textract_confidence_score', 0)
                    })
        return results

    # ‚ùå Skip all other combinations
    return []

# --- Query and Export ---
def query_and_export():
    paginator = dynamodb_client.get_paginator('scan')

    # Build filters based on pk flag
    if USE_TARGET_PK:
        filter_expr = '(#st = :status1 OR #st = :status2) AND #pk = :pkval'
        expr_values = {
            ':status1': {'S': 'Redacted'},
            ':status2': {'S': 'Manual Review'},
            ':pkval': {'S': TARGET_PK}
        }
        expr_names = {'#st': 'Status', '#pk': 'pk'}
    else:
        filter_expr = '#st = :status1 OR #st = :status2'
        expr_values = {
            ':status1': {'S': 'Redacted'},
            ':status2': {'S': 'Manual Review'}
        }
        expr_names = {'#st': 'Status'}

    # Write to CSV
    with open(CSV_FILE, mode='w', newline='') as csvfile:
        fieldnames = ['File Name', 'File Name No .PDF', 'Redacted Text', 'Redacted Type', 'BBox', 'Textract Confidence Score']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        for page in paginator.paginate(
            TableName=TABLE_NAME,
            FilterExpression=filter_expr,
            ExpressionAttributeValues=expr_values,
            ExpressionAttributeNames=expr_names
        ):
            items = page.get('Items', [])
            print(f"üîç Scanned page: {len(items)} items")

            for item in items:
                for row in process_item(item):
                    writer.writerow(row)

    print(f"\n‚úÖ CSV Export Complete: {CSV_FILE}")

# --- Run ---
if __name__ == "__main__":
    query_and_export()

##################################################


import os
import json
import boto3
import csv
from datetime import datetime

# === Global Configuration ===
script_dir = os.path.dirname(os.path.abspath(__file__))
config_path = os.getenv('CONFIG_PATH', os.path.join(script_dir, 'config_local_run.json'))
output_file = os.getenv('OUTPUT_FILE', os.path.join(script_dir, f'dynamodb_dump_{datetime.now().date()}.csv'))

# === Load Config ===
with open(config_path, 'r') as f:
    config = json.load(f)

# === AWS Credentials and Table Info ===
access_key = config['aws_prod']['access_key_prod']
secret_key = config['aws_prod']['secret_key_prod']
REGION = config['aws_prod']['region_prod']
table_name = config['services']['dynamodb']['table_name_prod']

# === Connect to DynamoDB ===
dynamodb = boto3.resource(
    'dynamodb',
    aws_access_key_id=access_key,
    aws_secret_access_key=secret_key,
    region_name=REGION
)

table = dynamodb.Table(table_name)

# === Scan Full Table ===
def scan_full_table():
    items = []
    start_key = None
    while True:
        if start_key:
            response = table.scan(ExclusiveStartKey=start_key)
        else:
            response = table.scan()

        items.extend(response.get('Items', []))
        start_key = response.get('LastEvaluatedKey')

        if not start_key:
            break

    return items

# === Dump Items to CSV ===
def dump_to_csv(items, output_path):
    # Discover all unique keys
    all_keys = set()
    for item in items:
        all_keys.update(item.keys())
    sorted_keys = sorted(all_keys)

    with open(output_path, 'w', newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=sorted_keys)
        writer.writeheader()
        for item in items:
            writer.writerow({key: item.get(key, '') for key in sorted_keys})

    print(f"‚úÖ DynamoDB table dumped to: {output_path}")
    print(f"üî¢ Total items written: {len(items)}")
    print(f"üóÇ Columns: {sorted_keys}")

# === Main ===
def main():
    print("üì• Scanning DynamoDB table...")
    items = scan_full_table()

    if not items:
        print("‚ö† No items found in the table.")
        return

    print("üíæ Writing to CSV...")
    dump_to_csv(items, output_file)

if __name__ == '__main__':
    main()



###################################

import os
import json
import boto3
import csv
from datetime import datetime

# === Config Paths ===
script_dir = os.path.dirname(os.path.abspath(__file__))
config_path = os.getenv('CONFIG_PATH', os.path.join(script_dir, 'config_local_run.json'))
output_file = os.getenv('OUTPUT_FILE', os.path.join(script_dir, f'daily_dynamo_report_{datetime.now().date()}.csv'))

# === Load Config ===
with open(config_path, 'r') as f:
    config = json.load(f)

# === AWS Setup ===
access_key = config['aws_prod']['access_key_prod']
secret_key = config['aws_prod']['secret_key_prod']
region = config['aws_prod']['region_prod']
table_name = config['services']['dynamodb']['table_name_prod']

dynamodb = boto3.resource(
    'dynamodb',
    aws_access_key_id=access_key,
    aws_secret_access_key=secret_key,
    region_name=region
)

table = dynamodb.Table(table_name)

# === Scan Function ===
def scan_table():
    items = []
    start_key = None

    while True:
        if start_key:
            response = table.scan(ExclusiveStartKey=start_key)
        else:
            response = table.scan()

        items.extend(response.get('Items', []))
        start_key = response.get('LastEvaluatedKey')
        if not start_key:
            break

    return items

def process_and_export(items, output_path):
    total_files = 0
    container_ids = set()
    redacted_files = 0
    sovereign_files = 0
    redacted_pages_total = 0

    csv_rows = []
    for item in items:
        status = item.get('Status', '').strip()
        pk = item.get('pk', '')
        metadata = item.get('Metadata', {})
        container_id = item.get('container_id', '')

        # Derived fields
        file_type = 'Non-Sovereign File' if status == 'Redacted' else 'Sovereign File'
        file_name = metadata.get('file_name', '')
        num_pages = len(metadata.get('pages', [])) if isinstance(metadata.get('pages', []), list) else 0

        # Metrics
        total_files += 1
        if container_id:
            container_ids.add(container_id)

        if file_type == 'Non-Sovereign File':
            redacted_files += 1
            redacted_pages_total += num_pages
        elif file_type == 'Sovereign File':
            sovereign_files += 1

        # Collect row for CSV
        csv_rows.append({
            'file_name': file_name,
            'pk': pk,
            'Status': status,
            'file_type': file_type,
            'num_pages': num_pages
        })

    # === Write CSV ===
    with open(output_path, 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=['file_name', 'pk', 'Status', 'file_type', 'num_pages'])
        writer.writeheader()
        writer.writerows(csv_rows)

    # === Metrics Summary ===
    print("\n DAILY STATUS REPORT")
    print(f"Total files processed        : {total_files}")
    print(f"Total containers used        : {len(container_ids)}")
    print(f"Non-Sovereign (Redacted)     : {redacted_files}")
    print(f"Sovereign (Manual Review)    : {sovereign_files}")
    print(f"Pages in redacted files      : {redacted_pages_total}")
    print(f"CSV written to               : {output_path}")

# === Main ===
def main():
    items = scan_table()
    if not items:
        print("No items found.")
        return
    process_and_export(items, output_file)

if __name__ == '__main__':
    main()

